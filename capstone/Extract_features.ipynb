{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load required packages\n",
    "from keras.applications import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import *\n",
    "from keras.layers import Dense,Flatten,Dropout,BatchNormalization\n",
    "import h5py as h\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/ec2-user/dogs_cats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_step(sample,batch_size):\n",
    "    if sample%batch_size == 0:\n",
    "        step = sample//batch_size\n",
    "    else:\n",
    "        step = sample//batch_size + 1\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract features\n",
    "def extract_features(model,path,gen_arg,input_size,batch_size):\n",
    "    #preprocessing data\n",
    "    gen = ImageDataGenerator(preprocessing_function = gen_arg)\n",
    "    train_gen = gen.flow_from_directory(path+'train',shuffle=False,target_size=(input_size,input_size),\n",
    "                                       batch_size=batch_size,class_mode='binary')\n",
    "    test_gen = gen.flow_from_directory(path+'test',shuffle=False,target_size=(input_size,input_size),\n",
    "                                      batch_size=batch_size,class_mode=None)\n",
    "    \n",
    "    #create pre-train model\n",
    "    base_model = model(weights='imagenet',include_top=False,pooling='avg')\n",
    "    pre_model = Model(inputs=base_model.input,outputs=base_model.output)\n",
    "    #extract features\n",
    "    trn_features = pre_model.predict_generator(train_gen,steps=compute_step(train_gen.n,batch_size),verbose=1)\n",
    "    test_features = pre_model.predict_generator(test_gen,steps=compute_step(test_gen.n,batch_size),verbose=1)\n",
    "    trn_label = to_categorical(train_gen.classes,num_classes=2)\n",
    "    #file_name = test_gen.filenames\n",
    "    return trn_features,test_features,trn_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(filename,x,test,y):\n",
    "    f = h.File(filename,'w')\n",
    "    f.create_dataset('x_train',data=x)\n",
    "    f.create_dataset('x_test',data=test)\n",
    "    f.create_dataset('y_train',data=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filename(gen=ImageDataGenerator()):\n",
    "    test_gen = gen.flow_from_directory(path+'test',shuffle=False,target_size=(224,224),\n",
    "                                      batch_size=64,class_mode=None)\n",
    "    return test_gen.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "391/391 [==============================] - 337s 862ms/step\n",
      "196/196 [==============================] - 130s 663ms/step\n"
     ]
    }
   ],
   "source": [
    "xception_x,xception_test,xception_y = extract_features(Xception,path,gen_arg=xception.preprocess_input,\n",
    "                                     input_size=299,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file('xception_features.h5',xception_x,xception_test,xception_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "391/391 [==============================] - 112s 287ms/step\n",
      "196/196 [==============================] - 55s 280ms/step\n"
     ]
    }
   ],
   "source": [
    "Inresnet_x,Inresnet_test,Inresnet_y = extract_features(InceptionResNetV2,path,gen_arg=inception_resnet_v2.preprocess_input,\n",
    "                                                      input_size=299,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file('Inresnet_features.h5',Inresnet_x,Inresnet_test,Inresnet_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "391/391 [==============================] - 87s 222ms/step\n",
      "196/196 [==============================] - 44s 223ms/step\n"
     ]
    }
   ],
   "source": [
    "resnet_x,resnet_test,resnet_y = extract_features(ResNet50,path,gen_arg=resnet50.preprocess_input,\n",
    "                                                input_size=224,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file('resnet_features.h5',resnet_x,resnet_test,resnet_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "76398592/76391848 [==============================] - 6s 0us/step\n",
      "391/391 [==============================] - 90s 231ms/step\n",
      "196/196 [==============================] - 44s 223ms/step\n"
     ]
    }
   ],
   "source": [
    "densenet_x,densenet_test,densenet_y = extract_features(DenseNet201,path,gen_arg=densenet.preprocess_input,\n",
    "                                                input_size=224,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file('DenseNet_features.h5',densenet_x,densenet_test,densenet_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data for new model\n",
    "x_train = np.concatenate((Inresnet_x,resnet_x,xception_x,densenet_x),axis=1)\n",
    "x_test = np.concatenate((Inresnet_test,resnet_test,xception_test,densenet_test),axis=1)\n",
    "y_train = resnet_y \n",
    "\n",
    "#x_train,y_train = shuffle(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 7552)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create classifier\n",
    "input_tensor = Input(x_train.shape[1:])\n",
    "#x = Dense(1024,activation='relu')(input_tensor)\n",
    "x = Dropout(0.8)(input_tensor)\n",
    "pred = Dense(2,activation='softmax')(x)\n",
    "model = Model(inputs=input_tensor,outputs=pred)\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/15\n",
      "20000/20000 [==============================] - 8s 417us/step - loss: 0.1012 - acc: 0.9627 - val_loss: 0.0274 - val_acc: 0.9922\n",
      "Epoch 2/15\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.0282 - acc: 0.9906 - val_loss: 0.0240 - val_acc: 0.9942\n",
      "Epoch 3/15\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0254 - acc: 0.9920 - val_loss: 0.0116 - val_acc: 0.9970\n",
      "Epoch 4/15\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0145 - val_acc: 0.9966\n",
      "Epoch 5/15\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.0136 - val_acc: 0.9968\n",
      "Epoch 6/15\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0199 - acc: 0.9940 - val_loss: 0.0105 - val_acc: 0.9970\n",
      "Epoch 7/15\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0205 - acc: 0.9937 - val_loss: 0.0199 - val_acc: 0.9962\n",
      "Epoch 8/15\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0187 - acc: 0.9940 - val_loss: 0.0173 - val_acc: 0.9964\n",
      "Epoch 9/15\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0195 - acc: 0.9941 - val_loss: 0.0185 - val_acc: 0.9958\n",
      "Epoch 10/15\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0207 - acc: 0.9940 - val_loss: 0.0169 - val_acc: 0.9962\n",
      "Epoch 11/15\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.0211 - acc: 0.9940 - val_loss: 0.0124 - val_acc: 0.9970\n",
      "Epoch 12/15\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0214 - acc: 0.9945 - val_loss: 0.0184 - val_acc: 0.9960\n",
      "Epoch 13/15\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0214 - acc: 0.9937 - val_loss: 0.0187 - val_acc: 0.9954\n",
      "Epoch 14/15\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0223 - val_acc: 0.9952\n",
      "Epoch 15/15\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0166 - acc: 0.9955 - val_loss: 0.0148 - val_acc: 0.9964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ad0d76128>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "model.fit(x_train,y_train,batch_size = 256,epochs = 15,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 4s 351us/step\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "prediction = model.predict(x_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clip before submiting\n",
    "pred = prediction.clip(min=0.005,max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_dog = pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "filenames = get_filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = [int(fname[fname.rfind('/')+1:fname.rfind('.')]) for fname in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = np.stack([index,is_dog],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_Inresnet_resnet_X_DenseNet = pd.DataFrame(sub,columns=['id','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_Inresnet_resnet_X_DenseNet.id = submission_Inresnet_resnet_X_DenseNet.id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_Inresnet_resnet_X_DenseNet.to_csv('submission_Inresnet_resnet_X_DenseNet.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
